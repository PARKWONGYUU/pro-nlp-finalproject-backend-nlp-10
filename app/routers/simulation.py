from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Dict, List, Optional
from datetime import timedelta, date
import logging
import math

from ..ml.prediction_service import get_prediction_service
from .. import crud, dataschemas
from ..database import get_db
from ..dummy_data_generator import get_generator
from app.config import settings

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/api",
    tags=["Simulation"]
)


def sanitize_float(value: float, default: float = 0.0) -> float:
    """
    ë¬´í•œëŒ€(inf) ë° NaN ê°’ì„ ì•ˆì „í•œ ê°’ìœ¼ë¡œ ë³€í™˜
    
    Args:
        value: ê²€ì‚¬í•  float ê°’
        default: inf/nanì¼ ê²½ìš° ë°˜í™˜í•  ê¸°ë³¸ê°’
    
    Returns:
        ì•ˆì „í•œ float ê°’
    """
    if math.isinf(value) or math.isnan(value):
        logger.warning(f"ë¬´í•œëŒ€/NaN ê°’ ê°ì§€: {value} -> {default}ë¡œ ëŒ€ì²´")
        return default
    return value


def sanitize_dict(data: dict) -> dict:
    """
    ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  float ê°’ì—ì„œ inf/nanì„ ì œê±°
    
    Args:
        data: ì •ë¦¬í•  ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ì •ë¦¬ëœ ë”•ì…”ë„ˆë¦¬
    """
    sanitized = {}
    for key, value in data.items():
        if isinstance(value, float):
            sanitized[key] = sanitize_float(value)
        elif isinstance(value, dict):
            sanitized[key] = sanitize_dict(value)
        elif isinstance(value, list):
            sanitized[key] = [
                sanitize_float(v) if isinstance(v, float) else v 
                for v in value
            ]
        else:
            sanitized[key] = value
    return sanitized


class SimulationValidator:
    """ì‹œë®¬ë ˆì´ì…˜ ìš”ì²­ ê²€ì¦"""
    
    # ì¡°ì • ê°€ëŠ¥í•œ Features (5ê°œ)
    VALID_FEATURES = {
        '10Y_Yield', 'USD_Index', 'pdsi', 'spi30d', 'spi90d'
    }
    
    @staticmethod
    def validate_feature_overrides(feature_overrides: Dict[str, float]) -> None:
        """Feature override ìœ íš¨ì„± ê²€ì¦"""
        invalid_features = set(feature_overrides.keys()) - SimulationValidator.VALID_FEATURES
        
        if invalid_features:
            raise HTTPException(
                status_code=400,
                detail=f"ì¡°ì • ë¶ˆê°€ëŠ¥í•œ feature: {invalid_features}. "
                       f"ê°€ëŠ¥í•œ features: {SimulationValidator.VALID_FEATURES}"
            )


class FeatureImpactCalculator:
    """Feature ì˜í–¥ë„ ê³„ì‚°"""
    
    @staticmethod
    def calculate_impacts(
        feature_overrides: Dict[str, float],
        historical_data: Dict[str, any],
        predictions: List[dataschemas.SimulationPredictionItem]
    ) -> List[dataschemas.FeatureImpact]:
        """
        ë³€ê²½ëœ Featureë“¤ì˜ ì˜í–¥ë„ ê³„ì‚°
        
        Args:
            feature_overrides: ì‚¬ìš©ìê°€ ë³€ê²½í•œ Featureë“¤
            historical_data: ê³¼ê±° ë°ì´í„°
            predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            Feature ì˜í–¥ë„ ë¦¬ìŠ¤íŠ¸ (ê¸°ì—¬ë„ í¬í•¨)
        """
        impacts = []
        
        # ì „ì²´ í‰ê·  ë³€í™”ëŸ‰
        total_avg_change = sum(p.change for p in predictions) / len(predictions) if predictions else 0
        
        # ê° featureì˜ ë³€í™”ëŸ‰ ë¹„ìœ¨ ê³„ì‚°
        total_abs_change = 0
        feature_changes = {}
        
        for feature_name, new_value in feature_overrides.items():
            current_value = FeatureImpactCalculator._get_current_value(
                feature_name, 
                historical_data
            )
            value_change = new_value - current_value
            abs_change = abs(value_change)
            
            feature_changes[feature_name] = {
                'current_value': current_value,
                'new_value': new_value,
                'value_change': value_change,
                'abs_change': abs_change
            }
            total_abs_change += abs_change
        
        # ê¸°ì—¬ë„ ê³„ì‚°: ì „ì²´ ë³€í™”ëŸ‰ì„ ê° featureì˜ ë³€í™” ë¹„ìœ¨ë¡œ ë°°ë¶„
        for feature_name, changes in feature_changes.items():
            if total_abs_change > 0:
                contribution_ratio = changes['abs_change'] / total_abs_change
                # ë³€í™” ë°©í–¥ ê³ ë ¤
                contribution = total_avg_change * contribution_ratio * (1 if changes['value_change'] >= 0 else -1)
            else:
                contribution = 0
            
            impacts.append(
                dataschemas.FeatureImpact(
                    feature=feature_name,
                    current_value=round(changes['current_value'], 2),
                    new_value=round(changes['new_value'], 2),
                    value_change=round(changes['value_change'], 2),
                    contribution=round(contribution, 2)
                )
            )
        
        return impacts
    
    @staticmethod
    def _get_current_value(feature_name: str, historical_data: Dict[str, any]) -> float:
        """í˜„ì¬(ìµœê·¼) Feature ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if feature_name in historical_data['features']:
            current_values = historical_data['features'][feature_name]
            return current_values[-1] if current_values else 0.0
        return 0.0


@router.post("/simulate", response_model=dataschemas.SimulationResponse)
def simulate_prediction(
    request: dataschemas.SimulationRequest,
    db: Session = Depends(get_db)
):
    """
    TFT ONNX ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ë¯¸ë˜ 60ì¼ ì˜ˆì¸¡)
    
    base_date ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±° 60ì¼ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ ,
    feature_overridesë¥¼ ì ìš©í•˜ì—¬ ë¯¸ë˜ 60ì¼ì„ ì¬ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    ì¡°ì • ê°€ëŠ¥í•œ Features (5ê°œ):
    - 10Y_Yield: ë¯¸êµ­ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬
    - USD_Index: ë‹¬ëŸ¬ ì¸ë±ìŠ¤
    - pdsi: Palmer Drought Severity Index
    - spi30d: Standardized Precipitation Index (30ì¼)
    - spi90d: Standardized Precipitation Index (90ì¼)
    """
    logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ - {request.commodity}, {request.base_date}")
    logger.info(f"ëª¨ë“œ: {settings.simulation_mode}")
    logger.info(f"Feature overrides: {request.feature_overrides}")
    
    # 1. Feature overrides ê²€ì¦
    SimulationValidator.validate_feature_overrides(request.feature_overrides)
    
    # 2. ë³€ê²½ ì‚¬í•­ í™•ì¸ (ë¹„ì–´ìˆê±°ë‚˜ ì˜ë¯¸ ìˆëŠ” ë³€ê²½ì´ ì—†ëŠ” ê²½ìš°)
    has_changes = False
    if request.feature_overrides:
        # ì—¬ê¸°ì— ì‹¤ì œ ê°’ê³¼ ë¹„êµí•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆì§€ë§Œ, 
        # ì¼ë‹¨ overridesê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
        # ë‹¨, ê°’ì´ ë¹„ì–´ìˆëŠ” dictì¸ ê²½ìš°ëŠ” ì œì™¸
        has_changes = len(request.feature_overrides) > 0
        
    # 3. ë³€ê²½ ì‚¬í•­ì´ ì—†ìœ¼ë©´ DB ì›ë³¸ ë°˜í™˜ (ê³„ì‚° ìƒëµ)
    if not has_changes and settings.simulation_mode == "dummy":
        logger.info("â„¹ï¸ ë³€ê²½ëœ ë³€ìˆ˜ê°€ ì—†ì–´ DB ì›ë³¸ ì˜ˆì¸¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return _get_original_predictions_response(request, db)
    
    # 4. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ë¶„ê¸°
    if settings.simulation_mode == "dummy":
        return _simulate_with_dummy(request, db)
    
    # 3. ONNX ëª¨ë“œ (ê¸°ì¡´ ë¡œì§)
    # 2. ê³¼ê±° ë°ì´í„° ë¡œë“œ
    historical_data = _load_historical_data(db, request)
    
    # 3. 60ì¼ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰ (ì›ë³¸ + ì‹œë®¬ë ˆì´ì…˜)
    predictions = _run_60day_predictions(request, historical_data)
    
    # 4. ìš”ì•½ í†µê³„ ê³„ì‚°
    summary = _calculate_summary(predictions)
    
    # 5. Feature ì˜í–¥ë„ ê³„ì‚° (ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë°˜)
    feature_impacts = FeatureImpactCalculator.calculate_impacts(
        request.feature_overrides,
        historical_data,
        predictions
    )
    
    # 6. Summaryì— featureë³„ ê¸°ì—¬ë„ ì¶”ê°€
    summary["feature_contributions"] = {
        impact.feature: impact.contribution 
        for impact in feature_impacts
    }
    summary["total_contribution"] = round(sum(impact.contribution for impact in feature_impacts), 2)
    
    # 7. inf/nan ê°’ ì •ë¦¬ (JSON ì§ë ¬í™” ì˜¤ë¥˜ ë°©ì§€)
    summary = sanitize_dict(summary)
    
    # 8. predictionsì˜ ê° í•­ëª©ë„ ì •ë¦¬
    for pred in predictions:
        pred.original_price = sanitize_float(pred.original_price)
        pred.simulated_price = sanitize_float(pred.simulated_price)
        pred.change = sanitize_float(pred.change)
        pred.change_percent = sanitize_float(pred.change_percent)
    
    # 9. feature_impactsë„ ì •ë¦¬
    for impact in feature_impacts:
        impact.current_value = sanitize_float(impact.current_value)
        impact.new_value = sanitize_float(impact.new_value)
        impact.value_change = sanitize_float(impact.value_change)
        impact.contribution = sanitize_float(impact.contribution)
    
    logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ - {len(predictions)}ì¼ì¹˜, í‰ê·  ë³€í™”: {summary.get('avg_change', 0):.2f}")
    
    return dataschemas.SimulationResponse(
        base_date=request.base_date.isoformat(),
        predictions=predictions,
        feature_impacts=feature_impacts,
        summary=summary
    )




def _load_historical_data(db: Session, request: dataschemas.SimulationRequest) -> Dict:
    """ê³¼ê±° 60ì¼ì˜ ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ"""
    try:
        historical_data = crud.get_historical_features(
            db, request.commodity, request.base_date, days=60
        )
        
        if not historical_data['dates']:
            raise HTTPException(
                status_code=404,
                detail=f"{request.commodity}ì˜ ê³¼ê±° 60ì¼ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. "
                       f"market_metrics í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ë¨¼ì € ì €ì¥í•˜ì„¸ìš”."
            )
        
        logger.info(f"ê³¼ê±° ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(historical_data['dates'])}ì¼")
        return historical_data
        
    except Exception as e:
        logger.error(f"ê³¼ê±° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ê³¼ê±° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )


def _run_60day_predictions(
    request: dataschemas.SimulationRequest,
    historical_data: Dict
) -> List[dataschemas.SimulationPredictionItem]:
    """
    ë¯¸ë˜ 60ì¼ ì˜ˆì¸¡ ì‹¤í–‰ (í•œ ë²ˆì—)
    
    ğŸ”¥ ë³€ê²½: Rolling Window ì œê±°, 60ì¼ í•œ ë²ˆì— ì˜ˆì¸¡
    - ì´ì „: 7ì¼ì”© 9ë²ˆ ë°˜ë³µ (ëˆ„ì  ì˜¤ì°¨ ë°œìƒ ê°€ëŠ¥)
    - í˜„ì¬: 60ì¼ í•œ ë²ˆì— ì˜ˆì¸¡ (ë°°ì¹˜ ì„œë²„ì™€ ë™ì¼)
    """
    pred_service = get_prediction_service()
    predictions = []
    
    try:
        # ì›ë³¸ ë° ì‹œë®¬ë ˆì´ì…˜ìš© historical data ë³µì‚¬
        original_hist = _copy_historical_data(historical_data)
        simulated_hist = _copy_historical_data(historical_data)
        
        # ğŸ”„ í˜„ì¬ ONNX ëª¨ë¸ì´ 7ì¼ìš©ì´ë¯€ë¡œ 9ë²ˆ ë°˜ë³µ í•„ìš”
        # TODO: 60ì¼ ONNX ëª¨ë¸ë¡œ êµì²´ í›„ num_cycles = 1ë¡œ ë³€ê²½
        num_cycles = (60 + 6) // 7  # 9 cycles (7-day predictions)
        
        for cycle in range(num_cycles):
            if len(predictions) >= 60:
                break
            
            logger.info(f"Rolling prediction cycle {cycle + 1}/{num_cycles}")
            
            # ì›ë³¸ ì˜ˆì¸¡ (override ì—†ì´)
            print(f"\n{'='*80}")
            print(f"ğŸ“Š Cycle {cycle + 1}/{num_cycles}: ì›ë³¸ ì˜ˆì¸¡ ìˆ˜í–‰ (override ì—†ìŒ)")
            print(f"{'='*80}")
            logger.info(f"ğŸ“Š ì›ë³¸ ì˜ˆì¸¡ ìˆ˜í–‰ (override ì—†ìŒ)")
            original_result = pred_service.predict_tft(
                request.commodity, 
                original_hist,
                feature_overrides=None
            )
            
            # ì‹œë®¬ë ˆì´ì…˜ ì˜ˆì¸¡ (override ì ìš©)
            print(f"\n{'='*80}")
            print(f"ğŸ“Š Cycle {cycle + 1}/{num_cycles}: ì‹œë®¬ë ˆì´ì…˜ ì˜ˆì¸¡ ìˆ˜í–‰")
            print(f"   Override: {request.feature_overrides}")
            print(f"{'='*80}")
            logger.info(f"ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ì˜ˆì¸¡ ìˆ˜í–‰ (override ì ìš©: {request.feature_overrides})")
            simulated_result = pred_service.predict_tft(
                request.commodity,
                simulated_hist,
                feature_overrides=request.feature_overrides
            )
            
            # ê²°ê³¼ ë¹„êµ ë¡œê¹…
            original_avg = sum(original_result['predictions']) / len(original_result['predictions'])
            simulated_avg = sum(simulated_result['predictions']) / len(simulated_result['predictions'])
            diff = simulated_avg - original_avg
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë° inf ë°©ì§€
            if original_avg != 0 and abs(original_avg) > 1e-10:
                diff_pct = sanitize_float((diff / original_avg) * 100)
            else:
                diff_pct = 0.0
            
            print(f"\nê²°ê³¼ ë¹„êµ:")
            print(f"  ì›ë³¸ í‰ê· : ${original_avg:.2f}")
            print(f"  ì‹œë®¬ í‰ê· : ${simulated_avg:.2f}")
            print(f"  ì°¨ì´: ${diff:.2f} ({diff_pct:.2f}%)")
            print(f"{'='*80}\n")
            
            logger.info(f"  ì›ë³¸ í‰ê·  ê°€ê²©: {original_avg:.2f}")
            logger.info(f"  ì‹œë®¬ë ˆì´ì…˜ í‰ê·  ê°€ê²©: {simulated_avg:.2f}")
            logger.info(f"  ì°¨ì´: {diff:.2f} ({diff_pct:.2f}%)")
            
            # 7ì¼ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
            original_prices = original_result['predictions']
            simulated_prices = simulated_result['predictions']
            
            for day_idx in range(len(original_prices)):
                if len(predictions) >= 60:
                    break
                
                pred_date = request.base_date + timedelta(days=len(predictions) + 1)
                original_price = sanitize_float(original_prices[day_idx])
                simulated_price = sanitize_float(simulated_prices[day_idx])
                
                change = simulated_price - original_price
                # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ ë° inf ë°©ì§€
                if original_price != 0 and abs(original_price) > 1e-10:
                    change_percent = (change / original_price) * 100
                    change_percent = sanitize_float(change_percent)
                else:
                    change_percent = 0.0
                
                predictions.append(dataschemas.SimulationPredictionItem(
                    date=pred_date.isoformat(),
                    original_price=round(original_price, 2),
                    simulated_price=round(simulated_price, 2),
                    change=round(change, 2),
                    change_percent=round(change_percent, 2)
                ))
            
            # Rolling window: ì˜ˆì¸¡ëœ 7ì¼ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            if len(predictions) < 60:
                logger.info(f"ğŸ”„ Rolling window ì—…ë°ì´íŠ¸ ì‹œì‘ (cycle {cycle + 1})")
                logger.info(f"   ì›ë³¸: ì˜ˆì¸¡ {len(original_prices)}ì¼ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì¶”ê°€")
                _update_historical_data_with_predictions(
                    original_hist, 
                    original_prices, 
                    None
                )
                logger.info(f"   ì‹œë®¬ë ˆì´ì…˜: ì˜ˆì¸¡ {len(simulated_prices)}ì¼ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì¶”ê°€ (override: {list(request.feature_overrides.keys())})")
                _update_historical_data_with_predictions(
                    simulated_hist, 
                    simulated_prices, 
                    request.feature_overrides
                )
                logger.info(f"ğŸ”„ Rolling window ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        return predictions[:60]  # ì •í™•íˆ 60ì¼ë§Œ ë°˜í™˜
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ì˜ˆì¸¡ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )


def _copy_historical_data(historical_data: Dict) -> Dict:
    """Historical data ê¹Šì€ ë³µì‚¬"""
    import copy
    return {
        'dates': historical_data['dates'].copy(),
        'features': {k: v.copy() for k, v in historical_data['features'].items()}
    }


def _update_historical_data_with_predictions(
    historical_data: Dict,
    predicted_prices: List[float],
    feature_overrides: Optional[Dict[str, float]]
):
    """
    ì˜ˆì¸¡ëœ ê°€ê²©ìœ¼ë¡œ historical data ì—…ë°ì´íŠ¸ (rolling window)
    
    - ê°€ì¥ ì˜¤ë˜ëœ Nì¼ ì œê±°
    - ì˜ˆì¸¡ëœ Nì¼ ì¶”ê°€
    - 60ì¼ window ìœ ì§€
    """
    num_days = len(predicted_prices)
    
    logger.info(f"  ğŸ“ Rolling window ìƒì„¸: {num_days}ì¼ ì˜ˆì¸¡ì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©")
    if feature_overrides:
        logger.info(f"     Override ì ìš©: {feature_overrides}")
    
    # ë‚ ì§œ ì—…ë°ì´íŠ¸ (ê°€ì¥ ì˜¤ë˜ëœ Nì¼ ì œê±°í•˜ê³  ìƒˆ ë‚ ì§œ ì¶”ê°€)
    old_dates_count = len(historical_data['dates'])
    historical_data['dates'] = historical_data['dates'][num_days:]
    logger.debug(f"     ë‚ ì§œ: {old_dates_count}ì¼ â†’ {len(historical_data['dates'])}ì¼ (ìµœê·¼ {num_days}ì¼ ì œê±°)")
    
    # ìŠ¬ë¼ì´ì‹± ì „ì— í•„ìš”í•œ ê°’ë“¤ì„ ë¯¸ë¦¬ ì €ì¥ (ìˆœì„œ ë¬¸ì œ ë°©ì§€)
    old_features = {
        name: values.copy() 
        for name, values in historical_data['features'].items()
    }
    
    # ê° feature ì—…ë°ì´íŠ¸
    for feature_name in historical_data['features']:
        feature_values = old_features[feature_name]
        
        # ìµœì†Œ í•„ìš” ê¸¸ì´ í™•ì¸ (60ì¼ ìœ ì§€ í•„ìš”)
        if len(feature_values) < num_days:
            logger.warning(f"âš ï¸ feature '{feature_name}'ì˜ ë°ì´í„°ê°€ {len(feature_values)}ì¼ë¡œ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ {num_days}ì¼ í•„ìš”). ì›ë³¸ ìœ ì§€")
            historical_data['features'][feature_name] = feature_values
            continue
        
        # ê°€ì¥ ì˜¤ë˜ëœ Nì¼ ì œê±°
        feature_values_trimmed = feature_values[num_days:]
        
        # ì˜ˆì¸¡ ê¸°ë°˜ ìƒˆ ê°’ ìƒì„±
        if feature_name == 'close':
            # ê°€ê²©: ì˜ˆì¸¡ê°’ ì‚¬ìš©
            new_values = predicted_prices
        elif feature_name in ['open', 'high', 'low']:
            # ê°€ê²© ê´€ë ¨: close ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
            # ìµœê·¼ closeì™€ì˜ ë¹„ìœ¨ì„ ìœ ì§€
            old_close_last = old_features['close'][-1]
            old_feature_last = feature_values[-1]
            
            if old_close_last != 0:
                ratio = old_feature_last / old_close_last
                new_values = [p * ratio for p in predicted_prices]
            else:
                new_values = predicted_prices
        elif feature_name == 'volume':
            # ê±°ë˜ëŸ‰: ìµœê·¼ í‰ê·  ìœ ì§€
            recent_values = feature_values[-min(7, len(feature_values)):]
            avg_volume = sum(recent_values) / len(recent_values) if recent_values else 0
            new_values = [avg_volume] * num_days
        elif feature_overrides and feature_name in feature_overrides:
            # Overrideëœ feature: ê³ ì •ê°’ ì‚¬ìš©
            override_value = feature_overrides[feature_name]
            new_values = [override_value] * num_days
            logger.debug(f"  ğŸ”§ Rolling window - {feature_name}: override ê°’ {override_value} ì ìš© ({num_days}ì¼)")
        else:
            # ê¸°íƒ€ feature: ë§ˆì§€ë§‰ ê°’ ìœ ì§€
            last_value = feature_values[-1] if len(feature_values) > 0 else 0
            new_values = [last_value] * num_days
        
        # ì—…ë°ì´íŠ¸ í›„ ê¸¸ì´ í™•ì¸
        updated_values = feature_values_trimmed + new_values
        historical_data['features'][feature_name] = updated_values
        
        # ë””ë²„ê¹…: ì£¼ìš” feature ë¡œê·¸
        if feature_name == 'close':
            logger.debug(f"     close: {len(feature_values)} â†’ {len(updated_values)}ì¼ (ì œê±° {num_days}, ì¶”ê°€ {num_days})")
            logger.debug(f"           ì¶”ê°€ëœ ê°’: {[round(v, 2) for v in new_values[:3]]}...")
        elif feature_overrides and feature_name in feature_overrides:
            logger.debug(f"     {feature_name}: override ê°’ {feature_overrides[feature_name]} ì ìš©ë¨ ({num_days}ì¼)")
    
    # ì—…ë°ì´íŠ¸ ì™„ë£Œ ë¡œê·¸
    total_features = len(historical_data['features'])
    override_count = len(feature_overrides) if feature_overrides else 0
    logger.debug(f"  âœ… Rolling window ì—…ë°ì´íŠ¸ ì™„ë£Œ: {total_features}ê°œ feature, {override_count}ê°œ override ì ìš©")


def _calculate_summary(predictions: List[dataschemas.SimulationPredictionItem]) -> dict:
    """ì „ì²´ ì˜ˆì¸¡ì˜ ìš”ì•½ í†µê³„ ê³„ì‚° (inf/nan ë°©ì§€)"""
    if not predictions:
        return {}
    
    # ì•ˆì „í•œ í•©ê³„ ê³„ì‚° (inf/nan í•„í„°ë§)
    total_change = sum(sanitize_float(p.change) for p in predictions)
    avg_change = total_change / len(predictions) if len(predictions) > 0 else 0
    
    total_change_percent = sum(sanitize_float(p.change_percent) for p in predictions)
    avg_change_percent = total_change_percent / len(predictions) if len(predictions) > 0 else 0
    
    avg_original = sum(sanitize_float(p.original_price) for p in predictions) / len(predictions) if len(predictions) > 0 else 0
    avg_simulated = sum(sanitize_float(p.simulated_price) for p in predictions) / len(predictions) if len(predictions) > 0 else 0
    
    # ì•ˆì „í•œ min/max ê³„ì‚°
    changes = [sanitize_float(p.change) for p in predictions]
    max_change = max(changes) if changes else 0
    min_change = min(changes) if changes else 0
    
    # max/min date ì°¾ê¸°
    max_pred = max(predictions, key=lambda p: sanitize_float(p.change))
    min_pred = min(predictions, key=lambda p: sanitize_float(p.change))
    
    return {
        "total_days": len(predictions),
        "avg_original_price": round(sanitize_float(avg_original), 2),
        "avg_simulated_price": round(sanitize_float(avg_simulated), 2),
        "avg_change": round(sanitize_float(avg_change), 2),
        "avg_change_percent": round(sanitize_float(avg_change_percent), 2),
        "total_change": round(sanitize_float(total_change), 2),
        "max_change": round(sanitize_float(max_change), 2),
        "min_change": round(sanitize_float(min_change), 2),
        "max_change_date": max_pred.date,
        "min_change_date": min_pred.date
    }


# ===========================
# ë”ë¯¸ ì‹œë®¬ë ˆì´ì…˜ (ì‹œì—°ìš©)
# ===========================

def _get_original_predictions_response(
    request: dataschemas.SimulationRequest,
    db: Session
) -> dataschemas.SimulationResponse:
    """
    ë³€ê²½ ì‚¬í•­ì´ ì—†ì„ ë•Œ DBì˜ ì›ë³¸ ì˜ˆì¸¡ê°’ì„ ë°˜í™˜
    """
    # 1. DBì—ì„œ ì›ë³¸ ì˜ˆì¸¡ ê°€ì ¸ì˜¤ê¸°
    today = request.base_date
    end_date = today + timedelta(days=60)
    
    original_predictions = crud.get_tft_predictions(
        db, 
        request.commodity, 
        start_date=today + timedelta(days=1),
        end_date=end_date
    )
    
    # 2. Response í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë³€í™”ëŸ‰ 0)
    predictions = []
    for i, pred in enumerate(original_predictions):
        if i >= 60: break
        price = float(pred.price_pred)
        predictions.append(dataschemas.SimulationPredictionItem(
            date=pred.target_date.isoformat(),
            original_price=price,
            simulated_price=price,
            change=0.0,
            change_percent=0.0
        ))
    
    # 3. ë¶€ì¡±í•œ ì¼ìˆ˜ëŠ” ë”ë¯¸ë¡œ ì±„ì›€ (ë§Œì•½ DB ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë‹¤ë©´)
    if len(predictions) < 60:
        generator = get_generator()
        dummy_preds = generator.generate_simulation_result(
            base_date=request.base_date,
            original_predictions=original_predictions,
            feature_overrides={},
            days=60
        )
        # ì´ë¯¸ ìˆëŠ” ë‚ ì§œëŠ” ìœ ì§€í•˜ê³  ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ ì±„ì›€
        existing_dates = set(p.date for p in predictions)
        for d_pred in dummy_preds:
            if d_pred.date not in existing_dates:
                # ë³€í™”ëŸ‰ 0ìœ¼ë¡œ ê°•ì œ ì„¤ì •
                d_pred.simulated_price = d_pred.original_price
                d_pred.change = 0.0
                d_pred.change_percent = 0.0
                predictions.append(d_pred)
        
        # ë‚ ì§œìˆœ ì •ë ¬
        predictions.sort(key=lambda x: x.date)
    
    # 4. Summary ìƒì„± (ë³€í™”ëŸ‰ 0)
    avg_price = sum(p.original_price for p in predictions) / len(predictions) if predictions else 0
    summary = {
        "total_days": len(predictions),
        "avg_original_price": round(avg_price, 2),
        "avg_simulated_price": round(avg_price, 2),
        "avg_change": 0.0,
        "avg_change_percent": 0.0,
        "total_change": 0.0,
        "max_change": 0.0,
        "min_change": 0.0,
        "feature_contributions": {},
        "total_contribution": 0.0
    }
    
    return dataschemas.SimulationResponse(
        base_date=request.base_date.isoformat(),
        predictions=predictions,
        feature_impacts=[],
        summary=summary
    )


def _simulate_with_dummy(
    request: dataschemas.SimulationRequest,
    db: Session
) -> dataschemas.SimulationResponse:
    """
    ë¹ ë¥¸ ë”ë¯¸ ì‹œë®¬ë ˆì´ì…˜ (ì‹œì—°ìš©)
    
    ONNX ëª¨ë¸ ëŒ€ì‹  ê°„ë‹¨í•œ ì„ í˜• ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ì‘ë‹µ
    - DBì—ì„œ ì›ë³¸ ì˜ˆì¸¡ ê°€ì ¸ì˜¤ê¸°
    - feature_override ê¸°ë°˜ìœ¼ë¡œ ê°€ê²© ë³€í™” ê³„ì‚°
    - 60ì¼ ì˜ˆì¸¡ ì¦‰ì‹œ ë°˜í™˜
    
    Args:
        request: ì‹œë®¬ë ˆì´ì…˜ ìš”ì²­
        db: DB ì„¸ì…˜
    
    Returns:
        SimulationResponse
    """
    logger.info("ğŸš€ ë”ë¯¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì‹œì‘ (ë¹ ë¥¸ ì‘ë‹µ)")
    
    # 0. í˜„ì¬ Feature ê°’ ì¡°íšŒ (Baselineìœ¼ë¡œ ì‚¬ìš©)
    current_values = {}
    try:
        # ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ ê°€ì ¸ì™€ì„œ ë§ˆì§€ë§‰ ê°’ í™•ì¸
        hist_data = crud.get_historical_features(
            db, request.commodity, request.base_date, days=30
        )
        if hist_data and hist_data.get('features'):
            for feat, vals in hist_data['features'].items():
                if vals:
                    current_values[feat] = vals[-1]
            logger.info(f"í˜„ì¬ Feature ê°’ ì¡°íšŒ ì„±ê³µ: {list(current_values.keys())}")
    except Exception as e:
        logger.warning(f"í˜„ì¬ Feature ê°’ ì¡°íšŒ ì‹¤íŒ¨ (ê¸°ë³¸ê°’ ì‚¬ìš©): {e}")

    # 1. DBì—ì„œ ì›ë³¸ ì˜ˆì¸¡ ê°€ì ¸ì˜¤ê¸°
    today = request.base_date
    end_date = today + timedelta(days=60)
    
    original_predictions = crud.get_tft_predictions(
        db, 
        request.commodity, 
        start_date=today + timedelta(days=1),
        end_date=end_date
    )
    
    logger.info(f"ì›ë³¸ ì˜ˆì¸¡ {len(original_predictions)}ê°œ ì¡°íšŒ ì™„ë£Œ")
    
    # 2. ë”ë¯¸ ë°ì´í„° ìƒì„±ê¸°ë¡œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
    generator = get_generator()
    
    predictions = generator.generate_simulation_result(
        base_date=request.base_date,
        original_predictions=original_predictions,
        feature_overrides=request.feature_overrides,
        days=60,
        current_values=current_values  # í˜„ì¬ ê°’ ì „ë‹¬
    )
    
    logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ì˜ˆì¸¡ {len(predictions)}ê°œ ìƒì„± ì™„ë£Œ")
    
    # 3. ìš”ì•½ í†µê³„ ê³„ì‚°
    summary = _calculate_summary(predictions)
    
    # 4. Feature ì˜í–¥ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
    feature_impacts = _calculate_dummy_feature_impacts(
        request.feature_overrides,
        predictions,
        current_values  # í˜„ì¬ ê°’ ì „ë‹¬
    )
    
    # 5. Summaryì— featureë³„ ê¸°ì—¬ë„ ì¶”ê°€
    summary["feature_contributions"] = {
        impact.feature: impact.contribution 
        for impact in feature_impacts
    }
    summary["total_contribution"] = round(sum(impact.contribution for impact in feature_impacts), 2)
    
    logger.info(f"âœ… ë”ë¯¸ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ - í‰ê·  ë³€í™”: {summary['avg_change']:.2f}")
    
    return dataschemas.SimulationResponse(
        base_date=request.base_date.isoformat(),
        predictions=predictions,
        feature_impacts=feature_impacts,
        summary=summary
    )


def _calculate_dummy_feature_impacts(
    feature_overrides: Dict[str, float],
    predictions: List[dataschemas.SimulationPredictionItem],
    current_values: Dict[str, float] = None
) -> List[dataschemas.FeatureImpact]:
    """
    ë”ë¯¸ ëª¨ë“œì˜ Feature ì˜í–¥ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
    
    Args:
        feature_overrides: ë³€ê²½í•œ featureë“¤
        predictions: ì˜ˆì¸¡ ê²°ê³¼
        current_values: í˜„ì¬ feature ê°’ë“¤
    
    Returns:
        FeatureImpact ë¦¬ìŠ¤íŠ¸
    """
    impacts = []
    if current_values is None:
        current_values = {}
    
    # ì „ì²´ í‰ê·  ë³€í™”ëŸ‰
    total_avg_change = sum(p.change for p in predictions) / len(predictions) if predictions else 0
    
    # Featureë³„ ê¸°ì¤€ê°’ (í˜„ì¬ ê°’ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    DEFAULT_BASELINES = {
        '10Y_Yield': 4.0,
        'USD_Index': 100.0,
        'pdsi': 0.0,
        'spi30d': 0.0,
        'spi90d': 0.0,
    }
    
    # ê° featureì˜ ë³€í™”ëŸ‰ ë¹„ìœ¨ ê³„ì‚°
    total_abs_change = 0
    feature_changes = {}
    
    for feature_name, new_value in feature_overrides.items():
        # DBì—ì„œ ê°€ì ¸ì˜¨ í˜„ì¬ ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        current_value = current_values.get(feature_name, DEFAULT_BASELINES.get(feature_name, 0.0))
        
        value_change = new_value - current_value
        abs_change = abs(value_change)
        
        feature_changes[feature_name] = {
            'current_value': current_value,
            'new_value': new_value,
            'value_change': value_change,
            'abs_change': abs_change
        }
        total_abs_change += abs_change
    
    # ê¸°ì—¬ë„ ê³„ì‚°: ì „ì²´ ë³€í™”ëŸ‰ì„ ê° featureì˜ ë³€í™” ë¹„ìœ¨ë¡œ ë°°ë¶„
    for feature_name, changes in feature_changes.items():
        if total_abs_change > 0:
            contribution_ratio = changes['abs_change'] / total_abs_change
            # ë³€í™” ë°©í–¥ ê³ ë ¤
            contribution = total_avg_change * contribution_ratio * (1 if changes['value_change'] >= 0 else -1)
        else:
            contribution = 0
        
        impacts.append(
            dataschemas.FeatureImpact(
                feature=feature_name,
                current_value=round(changes['current_value'], 2),
                new_value=round(changes['new_value'], 2),
                value_change=round(changes['value_change'], 2),
                contribution=round(contribution, 2)
            )
        )
    
    return impacts

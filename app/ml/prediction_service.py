import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import logging

from .model_loader import get_model_loader
from .lightweight_scaler import LightweightScaler
from app.config import settings

logger = logging.getLogger(__name__)


class TFTFeatureConfig:
    """TFT ëª¨ë¸ì˜ Feature êµ¬ì„± ì •ë³´"""
    
    # Feature ìˆœì„œ ì •ì˜ (ì´ 52ê°œ)
    FEATURE_ORDER = [
        # Unknown Time-Varying (46ê°œ)
        'close', 'open', 'high', 'low', 'volume', 'EMA',  # ê°€ê²©/ê±°ë˜ëŸ‰ 6ê°œ
        *[f'news_pca_{i}' for i in range(32)],  # ë‰´ìŠ¤ PCA 32ê°œ
        'pdsi', 'spi30d', 'spi90d',  # ê¸°í›„ ì§€ìˆ˜ 3ê°œ
        '10Y_Yield', 'USD_Index',  # ê±°ì‹œê²½ì œ 2ê°œ
        'lambda_price', 'lambda_news',  # Hawkes Intensity 2ê°œ
        'news_count',  # ë‰´ìŠ¤ ê°œìˆ˜ 1ê°œ
        # Known Time-Varying (3ê°œ)
        'time_idx', 'day_of_year', 'relative_time_idx',
        # Static (3ê°œ)
        'encoder_length', 'close_center', 'close_scale'
    ]
    
    # ì‹œê³„ì—´ ê´€ë ¨ Feature (ë™ì  ìƒì„± í•„ìš”)
    TIME_FEATURES = {'time_idx', 'day_of_year', 'relative_time_idx'}
    
    # Static Feature (ëª¨ë“  ì‹œì ì—ì„œ ë™ì¼)
    STATIC_FEATURES = {'encoder_length', 'close_center', 'close_scale'}
    
    # Known Future Features (ë¯¸ë˜ ì‹œì ì—ë„ ì•Œ ìˆ˜ ìˆëŠ” Feature)
    KNOWN_FEATURES = TIME_FEATURES | STATIC_FEATURES
    
    # Encoder/Decoder ê¸¸ì´
    ENCODER_LENGTH = 60
    DECODER_LENGTH = 7  # ğŸ”„ Temporarily reverted to 7 (current ONNX model limitation)
    
    # ê¸°ë³¸ê°’ (fallback)
    DEFAULT_CLOSE_VALUE = 450.0
    DEFAULT_SCALE_VALUE = 1.0
    DEFAULT_TARGET_CENTER = 450.0
    DEFAULT_TARGET_SCALE = 10.0
    
    # ì •ê·œí™”ì—ì„œ ì œì™¸í•  Featureë“¤ (ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆê±°ë‚˜ ë²”ìœ„ê°€ ê³ ì •ëœ ê²½ìš°)
    NORMALIZATION_EXCLUDE = TIME_FEATURES | STATIC_FEATURES | {'relative_time_idx'}


class ONNXPredictionService:
    """ONNX ê¸°ë°˜ TFT ëª¨ë¸ ì˜ˆì¸¡ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model_loader = get_model_loader()
        self.feature_config = TFTFeatureConfig()
        # ì •ê·œí™” íŒŒë¼ë¯¸í„° ìºì‹œ (commodityë³„ë¡œ ì €ì¥)
        self.normalization_params = {}
        # PKLì—ì„œ ë¡œë“œí•œ scaler ìºì‹œ
        self.pkl_scaler = None
        # LightweightScaler ìºì‹œ (GroupNormalizerìš©)
        self.lightweight_scaler = None
        # ì‚¬ìš© ì¤‘ì¸ ì •ê·œí™” ë°©ì‹
        self.normalization_method = None  # 'group_normalizer', 'standard_scaler', 'dynamic'
        logger.info("TFT ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def predict_tft(
        self, 
        commodity: str, 
        historical_data: Dict[str, any], 
        feature_overrides: Optional[Dict[str, float]] = None
    ) -> Dict[str, List[float]]:
        """
        TFT ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            commodity: í’ˆëª©ëª… (ì˜ˆ: "corn")
            historical_data: ê³¼ê±° ë°ì´í„°
                {
                    'dates': ['2024-01-01', '2024-01-02', ...],
                    'features': {
                        'close': [450.0, 451.0, ...],
                        'open': [449.0, 450.0, ...],
                        ...
                    }
                }
            feature_overrides: ì‚¬ìš©ì ë³€ê²½ Feature (ì„ íƒì‚¬í•­)
                {
                    "10Y_Yield": 4.5,
                    "USD_Index": 105.0,
                    ...
                }
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
            {
                'predictions': [ê°€ê²©1, ê°€ê²©2, ..., ê°€ê²©7],  # 7ì¼ ì˜ˆì¸¡ (ì¤‘ì•™ê°’)
                'lower_bounds': [...],  # í•˜í•œ
                'upper_bounds': [...]   # ìƒí•œ
            }
        """
        # ONNX ì„¸ì…˜ ë¡œë“œ
        session = self.model_loader.load_session(commodity)
        
        # TFT ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        model_inputs = self._prepare_model_inputs(historical_data, feature_overrides)
        
        # ë¡œê¹…
        self._log_inference_info(model_inputs)
        
        # ì¶”ë¡  ì‹¤í–‰
        outputs = session.run(None, model_inputs)
        
        # ê²°ê³¼ íŒŒì‹±
        result = self._parse_predictions(outputs)
        
        logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ - 7ì¼ ì˜ˆì¸¡: {result['predictions']}")
        
        return result
    
    def _prepare_model_inputs(
        self, 
        historical_data: Dict[str, any],
        feature_overrides: Optional[Dict[str, float]] = None
    ) -> Dict[str, np.ndarray]:
        """
        ê³¼ê±° ë°ì´í„°ë¥¼ TFT ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        TFT ì…ë ¥ êµ¬ì¡°:
        - encoder_cat: [1, 60, 1] - ê³¼ê±° 60ì¼ì˜ ë²”ì£¼í˜• (group_id)
        - encoder_cont: [1, 60, 52] - ê³¼ê±° 60ì¼ì˜ 52ê°œ ì—°ì†í˜• feature
        - encoder_lengths: [1] - ì¸ì½”ë” ê¸¸ì´ (60)
        - decoder_cat: [1, 7, 1] - ë¯¸ë˜ 7ì¼ì˜ ë²”ì£¼í˜•
        - decoder_cont: [1, 7, 52] - ë¯¸ë˜ 7ì¼ì˜ 52ê°œ ì—°ì†í˜• feature
        - decoder_lengths: [1] - ë””ì½”ë” ê¸¸ì´ (7)
        - target_scale: [1, 2] - íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° (center, scale)
        
        Returns:
            ONNX ëª¨ë¸ ì…ë ¥ í…ì„œ ë”•ì…”ë„ˆë¦¬
        """
        # ğŸ”¥ ì¤‘ìš”: Deep copyë¥¼ í•´ì•¼ ì›ë³¸ ë°ì´í„°ê°€ ë³€ê²½ë˜ì§€ ì•ŠìŒ!
        import copy
        features = copy.deepcopy(historical_data['features'])
        
        # ğŸ”¥ ì¶”ê°€: ëª¨ë¸ ì…ë ¥ìš© ë¡œê·¸ ë³€í™˜ (data_fetcherì—ì„œ ì œê±°í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ìˆ˜í–‰)
        # ê°€ê²© ê´€ë ¨ ë³€ìˆ˜ëŠ” log1p ë³€í™˜í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥
        log_cols = ['close', 'open', 'high', 'low', 'volume', 'EMA']
        for col in log_cols:
            if col in features and features[col]:
                # ë¦¬ìŠ¤íŠ¸ë¥¼ numpyë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° ì—°ì‚° í›„ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ
                arr = np.array(features[col], dtype=np.float64)
                # ì´ë¯¸ ë¡œê·¸ ë³€í™˜ëœ ê°’ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë³€í™˜ (ê°’ì˜ ë²”ìœ„ë¡œ ì¶”ì •)
                # ì˜¥ìˆ˜ìˆ˜ ê°€ê²©ì€ ë³´í†µ 400~600ì´ë¯€ë¡œ logê°’ì€ 6.0~6.5
                # ë§Œì•½ í‰ê· ì´ 10 ë¯¸ë§Œì´ë¼ë©´ ì´ë¯¸ ë¡œê·¸ ë³€í™˜ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ë„ ìˆì§€ë§Œ,
                # data_fetcherê°€ ì›ë³¸ì„ ì£¼ë„ë¡ ìˆ˜ì •í–ˆìœ¼ë¯€ë¡œ ë¬´ì¡°ê±´ ë³€í™˜
                features[col] = np.log1p(arr).tolist()
                logger.debug(f"   {col}: Log1p ë³€í™˜ ì™„ë£Œ (first={features[col][0]:.2f})")
        
        # Feature override ì ìš©
        if feature_overrides:
            features = self._apply_feature_overrides(features, feature_overrides)
        
        # ğŸ”¥ ì •ê·œí™” íŒŒë¼ë¯¸í„° ë¡œë“œ ë˜ëŠ” ê³„ì‚°
        # 1ìˆœìœ„: PKL íŒŒì¼ì˜ scaler ì‚¬ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼)
        # 2ìˆœìœ„: ë™ì  ê³„ì‚° (encoder ë°ì´í„° ê¸°ë°˜)
        self._load_or_compute_normalization_params(features)
        
        # ğŸ”¥ Feature ì •ê·œí™” ì ìš©
        normalized_features = self._normalize_features(features)
        
        # Encoder/Decoder ë°ì´í„° ìƒì„±
        encoder_cont = self._build_encoder_features(normalized_features)
        decoder_cont = self._build_decoder_features(normalized_features)
        
        # ë²”ì£¼í˜• ë°ì´í„° (group_id)
        encoder_cat = np.zeros([1, self.feature_config.ENCODER_LENGTH, 1], dtype=np.int64)
        decoder_cat = np.zeros([1, self.feature_config.DECODER_LENGTH, 1], dtype=np.int64)
        
        # Lengths
        encoder_lengths = np.array([self.feature_config.ENCODER_LENGTH], dtype=np.int64)
        decoder_lengths = np.array([self.feature_config.DECODER_LENGTH], dtype=np.int64)
        
        # Target scale (ì •ê·œí™”ëœ close ê°’ ê¸°ë°˜)
        target_scale = self._get_target_scale(features)
        
        return {
            'encoder_cat': encoder_cat,
            'encoder_cont': encoder_cont,
            'encoder_lengths': encoder_lengths,
            'decoder_cat': decoder_cat,
            'decoder_cont': decoder_cont,
            'decoder_lengths': decoder_lengths,
            'target_scale': target_scale
        }
    
    def _load_or_compute_normalization_params(self, features: Dict[str, List[float]]):
        """
        ì •ê·œí™” íŒŒë¼ë¯¸í„° ë¡œë“œ ë˜ëŠ” ê³„ì‚°
        
        ìš°ì„ ìˆœìœ„:
        1. GroupNormalizer (pytorch-forecasting) - LightweightScaler ì‚¬ìš©
        2. StandardScaler (sklearn) - PKLì—ì„œ mean_/scale_ ì¶”ì¶œ
        3. ë™ì  ê³„ì‚° (fallback) - Encoder ë°ì´í„°ë¡œ ê³„ì‚°
        """
        # 1ìˆœìœ„: GroupNormalizer ì‹œë„
        if self._load_group_normalizer_from_pkl():
            logger.info("âœ… GroupNormalizer ì‚¬ìš© (pytorch-forecasting ë°©ì‹)")
            self.normalization_method = 'group_normalizer'
            return
        
        # 2ìˆœìœ„: StandardScaler ì‹œë„
        if self._load_normalization_params_from_pkl():
            logger.info("âœ… StandardScaler ì‚¬ìš© (sklearn ë°©ì‹)")
            self.normalization_method = 'standard_scaler'
            return
        
        # 3ìˆœìœ„: ë™ì  ê³„ì‚° (fallback)
        logger.warning("âš ï¸ PKL scaler ë¡œë“œ ì‹¤íŒ¨, encoder ë°ì´í„°ë¡œ ë™ì  ê³„ì‚°")
        self._compute_normalization_params(features)
        self.normalization_method = 'dynamic'
    
    def _load_group_normalizer_from_pkl(self) -> bool:
        """
        PKL íŒŒì¼ì—ì„œ GroupNormalizer ë¡œë“œ (pytorch-forecasting)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            preprocessing_info = self.model_loader.get_preprocessing_info()
            
            if not preprocessing_info:
                logger.debug("ì „ì²˜ë¦¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # GroupNormalizer ê´€ë ¨ í‚¤ ì°¾ê¸°
            normalizer = None
            normalizer_key = None
            
            for key in ['target_normalizer', 'normalizer', 'target_scaler']:
                if key in preprocessing_info:
                    normalizer = preprocessing_info[key]
                    normalizer_key = key
                    break
            
            if normalizer is None:
                logger.debug("GroupNormalizerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # GroupNormalizer íƒ€ì… í™•ì¸
            normalizer_type = type(normalizer).__name__
            if 'GroupNormalizer' not in normalizer_type:
                logger.debug(f"GroupNormalizerê°€ ì•„ë‹˜: {normalizer_type}")
                return False
            
            # LightweightScalerë¡œ ë³€í™˜
            scaler_params = self._extract_group_normalizer_params(normalizer, preprocessing_info)
            
            if scaler_params is None:
                logger.warning("GroupNormalizer íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹¤íŒ¨")
                return False
            
            # LightweightScaler ìƒì„±
            self.lightweight_scaler = LightweightScaler(scaler_params)
            
            logger.info(f"âœ… GroupNormalizer ë¡œë“œ ì„±ê³µ: {normalizer_key}")
            logger.info(f"   Transformation: {scaler_params['metadata'].get('transformation', 'none')}")
            logger.info(f"   Groups: {scaler_params['metadata'].get('groups', [])}")
            
            return True
            
        except Exception as e:
            logger.warning(f"GroupNormalizer ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _extract_group_normalizer_params(self, normalizer, preprocessing_info: dict) -> Optional[dict]:
        """
        GroupNormalizerì—ì„œ LightweightScalerìš© íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        
        Args:
            normalizer: GroupNormalizer ê°ì²´
            preprocessing_info: ì „ì²˜ë¦¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            LightweightScalerìš© íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
            scaler_params = {
                'metadata': {
                    'source': 'pkl',
                    'target': preprocessing_info.get('target', 'close'),
                    'transformation': getattr(normalizer, 'transformation', 'none'),
                    'groups': preprocessing_info.get('group_ids', [])
                },
                'normalizer_params': {}
            }
            
            # Center & Scale ì¶”ì¶œ
            if hasattr(normalizer, 'center_'):
                center = normalizer.center_
                if hasattr(center, 'cpu'):
                    center = center.cpu().numpy()
                scaler_params['normalizer_params']['center'] = center.tolist() if hasattr(center, 'tolist') else [float(center)]
            
            if hasattr(normalizer, 'scale_'):
                scale = normalizer.scale_
                if hasattr(scale, 'cpu'):
                    scale = scale.cpu().numpy()
                scaler_params['normalizer_params']['scale'] = scale.tolist() if hasattr(scale, 'tolist') else [float(scale)]
            
            # ê·¸ë£¹ë³„ í†µê³„ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
            group_statistics = {}
            if hasattr(normalizer, 'group_centers_') and hasattr(normalizer, 'group_scales_'):
                group_centers = normalizer.group_centers_
                group_scales = normalizer.group_scales_
                
                groups = preprocessing_info.get('group_ids', [])
                for i, group in enumerate(groups):
                    if i < len(group_centers) and i < len(group_scales):
                        group_statistics[str(group)] = {
                            'mean': float(group_centers[i]) if hasattr(group_centers[i], 'item') else float(group_centers[i]),
                            'std': float(group_scales[i]) if hasattr(group_scales[i], 'item') else float(group_scales[i])
                        }
            
            if group_statistics:
                scaler_params['normalizer_params']['group_statistics'] = group_statistics
            
            return scaler_params
            
        except Exception as e:
            logger.error(f"GroupNormalizer íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_normalization_params_from_pkl(self) -> bool:
        """
        PKL íŒŒì¼ì—ì„œ ì •ê·œí™” íŒŒë¼ë¯¸í„° ë¡œë“œ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì „ì²˜ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            preprocessing_info = self.model_loader.get_preprocessing_info()
            
            if not preprocessing_info:
                logger.debug("ì „ì²˜ë¦¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # Scaler ê°ì²´ ì°¾ê¸° (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ í™•ì¸)
            scaler = None
            for key in ['scaler', 'feature_scaler', 'x_scaler', 'normalizer']:
                if key in preprocessing_info:
                    scaler = preprocessing_info[key]
                    break
            
            if scaler is None:
                logger.debug("PKLì—ì„œ scalerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # StandardScalerì˜ mean_, scale_ ì†ì„± í™•ì¸
            if not (hasattr(scaler, 'mean_') and hasattr(scaler, 'scale_')):
                logger.warning(f"Scalerì— mean_/scale_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤: {type(scaler)}")
                return False
            
            # Feature ìˆœì„œ í™•ì¸ (PKLì— ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
            feature_names = None
            if hasattr(scaler, 'feature_names_in_'):
                feature_names = scaler.feature_names_in_
            elif 'feature_names' in preprocessing_info:
                feature_names = preprocessing_info['feature_names']
            
            # ì •ê·œí™” íŒŒë¼ë¯¸í„° ìƒì„±
            params = {}
            mean_array = np.array(scaler.mean_)
            scale_array = np.array(scaler.scale_)
            
            if feature_names is not None:
                # Feature ì´ë¦„ì´ ìˆëŠ” ê²½ìš° ë§¤í•‘
                for i, feature_name in enumerate(feature_names):
                    if i < len(mean_array) and i < len(scale_array):
                        params[feature_name] = {
                            'mean': float(mean_array[i]),
                            'std': float(scale_array[i])
                        }
            else:
                # Feature ì´ë¦„ì´ ì—†ëŠ” ê²½ìš°, FEATURE_ORDER ìˆœì„œëŒ€ë¡œ ë§¤í•‘
                logger.warning("Feature ì´ë¦„ì´ ì—†ì–´ FEATURE_ORDER ìˆœì„œë¡œ ë§¤í•‘í•©ë‹ˆë‹¤")
                idx = 0
                for feature_name in self.feature_config.FEATURE_ORDER:
                    if feature_name in self.feature_config.NORMALIZATION_EXCLUDE:
                        continue
                    if idx < len(mean_array) and idx < len(scale_array):
                        params[feature_name] = {
                            'mean': float(mean_array[idx]),
                            'std': float(scale_array[idx])
                        }
                        idx += 1
            
            # ìºì‹œì— ì €ì¥
            self.normalization_params = params
            self.pkl_scaler = scaler
            
            logger.info(f"âœ… PKL scaler ë¡œë“œ ì„±ê³µ: {len(params)}ê°œ feature")
            logger.debug(f"   ì˜ˆì‹œ: close = mean:{params.get('close', {}).get('mean', 0):.2f}, "
                        f"std:{params.get('close', {}).get('std', 1):.2f}")
            return True
            
        except Exception as e:
            logger.warning(f"PKL scaler ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _compute_normalization_params(self, features: Dict[str, List[float]]):
        """
        Encoder ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™” íŒŒë¼ë¯¸í„° ê³„ì‚° (StandardScaler ë°©ì‹)
        
        Fallback: PKL ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©
        """
        params = {}
        
        for feature_name in self.feature_config.FEATURE_ORDER:
            # ì •ê·œí™” ì œì™¸ ëŒ€ìƒì€ ê±´ë„ˆë›°ê¸°
            if feature_name in self.feature_config.NORMALIZATION_EXCLUDE:
                continue
            
            # featureê°€ ì¡´ì¬í•˜ê³  encoder ë²”ìœ„ ë‚´ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            if feature_name in features and len(features[feature_name]) >= self.feature_config.ENCODER_LENGTH:
                # Encoder êµ¬ê°„ë§Œ ì‚¬ìš© (ê³¼ê±° 60ì¼)
                encoder_values = features[feature_name][:self.feature_config.ENCODER_LENGTH]
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                values_array = np.array(encoder_values, dtype=np.float32)
                
                # meanê³¼ std ê³„ì‚°
                mean_val = np.mean(values_array)
                std_val = np.std(values_array)
                
                # stdê°€ 0ì´ë©´ 1ë¡œ ëŒ€ì²´ (division by zero ë°©ì§€)
                if std_val == 0 or np.isnan(std_val):
                    std_val = 1.0
                
                params[feature_name] = {
                    'mean': float(mean_val),
                    'std': float(std_val)
                }
        
        # ìºì‹œì— ì €ì¥
        self.normalization_params = params
        
        logger.info(f"ğŸ“Š ì •ê·œí™” íŒŒë¼ë¯¸í„° ë™ì  ê³„ì‚° ì™„ë£Œ: {len(params)}ê°œ feature")
        logger.debug(f"   ì˜ˆì‹œ: close = mean:{params.get('close', {}).get('mean', 0):.2f}, "
                    f"std:{params.get('close', {}).get('std', 1):.2f}")
    
    def _normalize_features(self, features: Dict[str, List[float]]) -> Dict[str, List[float]]:
        """
        Featureë“¤ì„ ì •ê·œí™”
        
        ì •ê·œí™” ë°©ì‹:
        - GroupNormalizer: LightweightScaler ì‚¬ìš© (softplus + group normalization)
        - StandardScaler: Z-score normalization ((x - mean) / std)
        - Dynamic: Encoder ë°ì´í„° ê¸°ë°˜ Z-score
        
        Returns:
            ì •ê·œí™”ëœ features ë”•ì…”ë„ˆë¦¬
        """
        # GroupNormalizer ì‚¬ìš©
        if self.normalization_method == 'group_normalizer':
            return self._normalize_with_group_normalizer(features)
        
        # StandardScaler ë˜ëŠ” Dynamic (ë™ì¼í•œ Z-score ë°©ì‹)
        normalized = {}
        
        for feature_name, values in features.items():
            # ì •ê·œí™” ì œì™¸ ëŒ€ìƒì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if feature_name in self.feature_config.NORMALIZATION_EXCLUDE:
                normalized[feature_name] = values
                continue
            
            # ì •ê·œí™” íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ê²½ìš° ì ìš©
            if feature_name in self.normalization_params:
                params = self.normalization_params[feature_name]
                mean_val = params['mean']
                std_val = params['std']
                
                # ì •ê·œí™” ì ìš©: (x - mean) / std
                normalized_values = [
                    (val - mean_val) / std_val 
                    for val in values
                ]
                normalized[feature_name] = normalized_values
                
                logger.debug(f"   {feature_name} ì •ê·œí™”: mean={mean_val:.2f}, std={std_val:.2f}")
            else:
                # íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                normalized[feature_name] = values
        
        logger.info(f"âœ… Feature ì •ê·œí™” ì™„ë£Œ ({self.normalization_method}): "
                   f"{len(self.normalization_params)}ê°œ ì •ê·œí™”ë¨")
        return normalized
    
    def _normalize_with_group_normalizer(self, features: Dict[str, List[float]]) -> Dict[str, List[float]]:
        """
        GroupNormalizer ë°©ì‹ìœ¼ë¡œ ì •ê·œí™” (LightweightScaler ì‚¬ìš©)
        
        Args:
            features: ì›ë³¸ features
            
        Returns:
            ì •ê·œí™”ëœ features
        """
        if self.lightweight_scaler is None:
            logger.error("LightweightScalerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return features
        
        normalized = {}
        group_id = "corn"  # ê¸°ë³¸ê°’, í•„ìš”ì‹œ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
        
        for feature_name, values in features.items():
            # ì •ê·œí™” ì œì™¸ ëŒ€ìƒ
            if feature_name in self.feature_config.NORMALIZATION_EXCLUDE:
                normalized[feature_name] = values
                continue
            
            # Target feature (close)ëŠ” GroupNormalizer ì ìš©
            if feature_name == 'close':
                try:
                    values_array = np.array(values, dtype=np.float32)
                    normalized_array = self.lightweight_scaler.transform(
                        values_array, 
                        group_id=group_id
                    )
                    normalized[feature_name] = normalized_array.tolist()
                    logger.debug(f"   {feature_name}: GroupNormalizer ì ìš©")
                except Exception as e:
                    logger.warning(f"   {feature_name}: GroupNormalizer ì ìš© ì‹¤íŒ¨ ({e}), ì›ë³¸ ì‚¬ìš©")
                    normalized[feature_name] = values
            else:
                # ë‹¤ë¥¸ featuresëŠ” ì¼ë°˜ ì •ê·œí™” (ìˆëŠ” ê²½ìš°)
                if feature_name in self.normalization_params:
                    params = self.normalization_params[feature_name]
                    mean_val = params['mean']
                    std_val = params['std']
                    
                    normalized_values = [
                        (val - mean_val) / std_val 
                        for val in values
                    ]
                    normalized[feature_name] = normalized_values
                else:
                    normalized[feature_name] = values
        
        logger.info(f"âœ… Feature ì •ê·œí™” ì™„ë£Œ (GroupNormalizer + StandardScaler)")
        return normalized
    
    def _apply_feature_overrides(
        self, 
        features: Dict[str, List[float]], 
        overrides: Dict[str, float]
    ) -> Dict[str, List[float]]:
        """Feature overrideë¥¼ ì ìš© (ì›ë³¸ì€ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ)"""
        print(f"\n{'='*80}")
        print(f"ğŸ”§ Feature override ì ìš© ì‹œì‘: {overrides}")
        logger.info(f"ğŸ”§ Feature override ì ìš© ì‹œì‘: {overrides}")
        
        # ì´ë¯¸ deep copyëœ featuresë¥¼ ë°›ì§€ë§Œ, ëª…í™•ì„±ì„ ìœ„í•´ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸
        for key, value in overrides.items():
            if key in features:
                original_value = features[key][-1] if features[key] else 0.0
                original_length = len(features[key])
                
                # ğŸ”¥ ì¤‘ìš”: ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ëª¨ë“  ì‹œì ì— ë™ì¼í•œ ê°’ ì ìš©)
                features[key] = [value] * original_length
                
                msg = f"  âœ“ {key}: {original_value:.2f} â†’ {value:.2f} (ë³€í™”: {value - original_value:.2f}, {original_length}ì¼)"
                print(msg)
                logger.info(msg)
            else:
                msg = f"  âš ï¸  {key}: featuresì— ì—†ìŒ (ë¬´ì‹œë¨)"
                print(msg)
                logger.warning(msg)
        
        msg = f"ğŸ”§ Feature override ì ìš© ì™„ë£Œ: {len(overrides)}ê°œ feature ë³€ê²½ë¨"
        print(msg)
        print(f"{'='*80}\n")
        logger.info(msg)
        return features
    
    def _build_encoder_features(self, features: Dict[str, List[float]]) -> np.ndarray:
        """Encoderìš© feature ë°°ì—´ ìƒì„± (ê³¼ê±° 60ì¼)"""
        encoder_data = []
        
        for i in range(self.feature_config.ENCODER_LENGTH):
            feature_vector = self._get_feature_vector_at_index(features, i, is_encoder=True)
            encoder_data.append(feature_vector)
        
        return np.array([encoder_data], dtype=np.float32)  # [1, 60, 52]
    
    def _build_decoder_features(self, features: Dict[str, List[float]]) -> np.ndarray:
        """Decoderìš© feature ë°°ì—´ ìƒì„± (ë¯¸ë˜ 7ì¼)"""
        decoder_data = []
        
        for i in range(self.feature_config.DECODER_LENGTH):
            feature_vector = self._get_feature_vector_at_index(
                features, 
                self.feature_config.ENCODER_LENGTH + i, 
                is_encoder=False
            )
            decoder_data.append(feature_vector)
        
        return np.array([decoder_data], dtype=np.float32)  # [1, 7, 52]
    
    def _get_feature_vector_at_index(
        self, 
        features: Dict[str, List[float]], 
        time_idx: int, 
        is_encoder: bool
    ) -> List[float]:
        """íŠ¹ì • ì‹œì ì˜ feature ë²¡í„° ìƒì„±"""
        feature_vector = []
        
        for fname in self.feature_config.FEATURE_ORDER:
            value = self._get_feature_value(features, fname, time_idx, is_encoder)
            feature_vector.append(value)
        
        return feature_vector
    
    def _get_feature_value(
        self, 
        features: Dict[str, List[float]], 
        feature_name: str, 
        time_idx: int, 
        is_encoder: bool
    ) -> float:
        """ê°œë³„ feature ê°’ ê³„ì‚°"""
        # Static Features
        if feature_name == 'encoder_length':
            return float(self.feature_config.ENCODER_LENGTH)
        elif feature_name == 'close_scale':
            return self.feature_config.DEFAULT_SCALE_VALUE
        elif feature_name == 'close_center':
            return self._get_close_value(features, time_idx)
        
        # Time Features
        elif feature_name == 'time_idx':
            return float(time_idx)
        elif feature_name == 'day_of_year':
            return self._get_day_of_year(time_idx, is_encoder)
        elif feature_name == 'relative_time_idx':
            total_length = self.feature_config.ENCODER_LENGTH + self.feature_config.DECODER_LENGTH
            return float(time_idx) / float(total_length)
        
        # Unknown Features (Decoderì—ì„œëŠ” 0)
        elif not is_encoder and feature_name not in self.feature_config.KNOWN_FEATURES:
            return 0.0
        
        # ì¼ë°˜ Features
        elif feature_name in features:
            if time_idx < len(features[feature_name]):
                return features[feature_name][time_idx]
            return 0.0
        
        # ê¸°ë³¸ê°’
        return 0.0
    
    def _get_close_value(self, features: Dict[str, List[float]], time_idx: int) -> float:
        """Close ê°€ê²© ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if 'close' in features and time_idx < len(features['close']):
            return features['close'][time_idx]
        return self.feature_config.DEFAULT_CLOSE_VALUE
    
    def _get_day_of_year(self, time_idx: int, is_encoder: bool) -> float:
        """ì—°ì¤‘ ëª‡ ë²ˆì§¸ ë‚ ì¸ì§€ ê³„ì‚°"""
        if is_encoder:
            # ê³¼ê±° ë‚ ì§œ
            days_ago = self.feature_config.ENCODER_LENGTH - time_idx
            target_date = datetime.now() - timedelta(days=days_ago)
        else:
            # ë¯¸ë˜ ë‚ ì§œ
            days_ahead = time_idx - self.feature_config.ENCODER_LENGTH
            target_date = datetime.now() + timedelta(days=days_ahead)
        
        return float(target_date.timetuple().tm_yday)
    
    def _get_target_scale(self, features: Dict[str, List[float]]) -> np.ndarray:
        """
        Target scale íŒŒë¼ë¯¸í„° ìƒì„±
        
        ì •ê·œí™” íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ê³„ì‚°
        - center: closeì˜ í‰ê· ê°’
        - scale: closeì˜ í‘œì¤€í¸ì°¨
        """
        # 1ìˆœìœ„: GroupNormalizer (LightweightScaler) ì‚¬ìš©
        if self.lightweight_scaler and 'close' in features:
            close_values = np.array(features['close'])
            center = float(np.mean(close_values))
            scale = float(np.std(close_values))
            
            logger.debug(f"ğŸ“Š Target scale: center={center:.2f}, scale={scale:.2f} (GroupNormalizer)")
            return np.array([[center, scale]], dtype=np.float32)
        
        # 2ìˆœìœ„: StandardScaler íŒŒë¼ë¯¸í„° ì‚¬ìš©
        if 'close' in self.normalization_params:
            params = self.normalization_params['close']
            center = params['mean']
            scale = params['std']
            
            logger.debug(f"ğŸ“Š Target scale: center={center:.2f}, scale={scale:.2f} (StandardScaler)")
            return np.array([[center, scale]], dtype=np.float32)
        
        # 3ìˆœìœ„: ë™ì  ê³„ì‚° (í˜„ì¬ ë°ì´í„° ê¸°ë°˜)
        if 'close' in features:
            close_values = np.array(features['close'])
            center = float(np.mean(close_values))
            scale = float(np.std(close_values))
            
            logger.debug(f"ğŸ“Š Target scale: center={center:.2f}, scale={scale:.2f} (ë™ì  ê³„ì‚°)")
            return np.array([[center, scale]], dtype=np.float32)
        
        # 4ìˆœìœ„ fallback: ê¸°ë³¸ê°’ ì‚¬ìš©
        center = self.feature_config.DEFAULT_TARGET_CENTER
        scale = self.feature_config.DEFAULT_TARGET_SCALE
        
        logger.warning(f"âš ï¸ Target scale: ê¸°ë³¸ê°’ ì‚¬ìš© (center={center}, scale={scale})")
        return np.array([[center, scale]], dtype=np.float32)
    
    def _parse_predictions(self, outputs: List[np.ndarray]) -> Dict[str, List[float]]:
        """
        ONNX ì¶œë ¥ì„ íŒŒì‹±í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜
        
        GroupNormalizerë¥¼ ì‚¬ìš©í•œ ê²½ìš° ì—­ë³€í™˜ ì ìš©
        """
        predictions = outputs[0]  # shape: [1, 7, 3]
        
        # ê¸°ë³¸ ì˜ˆì¸¡ê°’
        pred_median = predictions[0, :, 0]  # ì¤‘ì•™ê°’
        pred_lower = predictions[0, :, 1]   # í•˜í•œ
        pred_upper = predictions[0, :, 2]   # ìƒí•œ
        
        logger.info(f"ğŸ” ì˜ˆì¸¡ê°’ (ì—­ë³€í™˜ ì „): median[0]={pred_median[0]:.4f}, method={self.normalization_method}")
        
        # GroupNormalizer ì—­ë³€í™˜
        if self.normalization_method == 'group_normalizer' and self.lightweight_scaler is not None:
            try:
                group_id = "corn"  # ê¸°ë³¸ê°’
                
                # ì—­ë³€í™˜ ì ìš©
                pred_median = self.lightweight_scaler.inverse_transform(pred_median, group_id=group_id)
                pred_lower = self.lightweight_scaler.inverse_transform(pred_lower, group_id=group_id)
                pred_upper = self.lightweight_scaler.inverse_transform(pred_upper, group_id=group_id)
                
                logger.info(f"âœ… GroupNormalizer ì—­ë³€í™˜ ì™„ë£Œ: median[0]={pred_median[0]:.4f}")
            except Exception as e:
                logger.warning(f"GroupNormalizer ì—­ë³€í™˜ ì‹¤íŒ¨: {e}, ì›ë³¸ ì‚¬ìš©")
        else:
            logger.warning(f"âš ï¸ ì—­ë³€í™˜ ë¯¸ì ìš©: method={self.normalization_method}, scaler={'ìˆìŒ' if self.lightweight_scaler else 'ì—†ìŒ'}")
        
        # ğŸ”¥ ì¤‘ìš”: log1p ì—­ë³€í™˜ (ë°ì´í„°ê°€ log1pë¡œ ë³€í™˜ë˜ì—ˆìœ¼ë¯€ë¡œ)
        # ë°°ì¹˜ ì„œë²„ì˜ predict.pyì—ì„œë„ log1p ë³€í™˜ì„ í•˜ë¯€ë¡œ, ì—­ë³€í™˜ í•„ìš”
        pred_median = np.expm1(pred_median)
        pred_lower = np.expm1(pred_lower)
        pred_upper = np.expm1(pred_upper)
        
        logger.info(f"âœ… Log1p ì—­ë³€í™˜ ì™„ë£Œ: median[0]=${pred_median[0]:.2f}")
        
        return {
            'predictions': pred_median.tolist() if hasattr(pred_median, 'tolist') else list(pred_median),
            'lower_bounds': pred_lower.tolist() if hasattr(pred_lower, 'tolist') else list(pred_lower),
            'upper_bounds': pred_upper.tolist() if hasattr(pred_upper, 'tolist') else list(pred_upper)
        }
    
    def _log_inference_info(self, model_inputs: Dict[str, np.ndarray]):
        """ì¶”ë¡  ì •ë³´ ë¡œê¹…"""
        logger.info("TFT ì¶”ë¡  ì‹¤í–‰")
        for name, array in model_inputs.items():
            logger.info(f"  {name}: {array.shape}")


def get_prediction_service() -> ONNXPredictionService:
    """ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return ONNXPredictionService()
